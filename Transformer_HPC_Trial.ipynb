{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from scipy.io import wavfile as wav\n",
    "import os\n",
    "from datetime import datetime \n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_data='mfcc_label.xlsx'\n",
    "dffeatures=pd.read_excel(file_name_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71911</th>\n",
       "      <td>-80</td>\n",
       "      <td>-0.035597</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-8.098465</td>\n",
       "      <td>-8.980048</td>\n",
       "      <td>-15.847272</td>\n",
       "      <td>-10.322305</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-11.243167</td>\n",
       "      <td>-36.083233</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.291732</td>\n",
       "      <td>-12.942391</td>\n",
       "      <td>-36.452793</td>\n",
       "      <td>-17.051119</td>\n",
       "      <td>-60.872829</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-58.338337</td>\n",
       "      <td>-41.416588</td>\n",
       "      <td>-59.643997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71912</th>\n",
       "      <td>-80</td>\n",
       "      <td>-0.094599</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-7.325050</td>\n",
       "      <td>-11.943776</td>\n",
       "      <td>-17.820580</td>\n",
       "      <td>-8.446717</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-11.624997</td>\n",
       "      <td>-12.545907</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.534180</td>\n",
       "      <td>-16.903601</td>\n",
       "      <td>-36.371616</td>\n",
       "      <td>-16.394320</td>\n",
       "      <td>-42.433369</td>\n",
       "      <td>-59.528172</td>\n",
       "      <td>-61.072720</td>\n",
       "      <td>-60.711777</td>\n",
       "      <td>-40.487423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71913</th>\n",
       "      <td>-80</td>\n",
       "      <td>-0.152973</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-6.007671</td>\n",
       "      <td>-13.862395</td>\n",
       "      <td>-17.086370</td>\n",
       "      <td>-8.125309</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-11.100876</td>\n",
       "      <td>-13.036957</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.564272</td>\n",
       "      <td>-39.630966</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-38.364044</td>\n",
       "      <td>-43.945827</td>\n",
       "      <td>-17.266998</td>\n",
       "      <td>-39.512138</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-39.440868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71914</th>\n",
       "      <td>-80</td>\n",
       "      <td>-0.184401</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-6.244070</td>\n",
       "      <td>-38.443878</td>\n",
       "      <td>-58.667614</td>\n",
       "      <td>-9.242589</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-10.375988</td>\n",
       "      <td>-13.404880</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.364969</td>\n",
       "      <td>-35.572617</td>\n",
       "      <td>-36.778931</td>\n",
       "      <td>-16.553949</td>\n",
       "      <td>-64.693108</td>\n",
       "      <td>-59.632282</td>\n",
       "      <td>-43.261234</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71915</th>\n",
       "      <td>-80</td>\n",
       "      <td>-0.188906</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-6.216877</td>\n",
       "      <td>-57.667927</td>\n",
       "      <td>-57.141754</td>\n",
       "      <td>-8.873954</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-9.454443</td>\n",
       "      <td>-13.660504</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.978451</td>\n",
       "      <td>-13.844250</td>\n",
       "      <td>-36.613995</td>\n",
       "      <td>-36.456799</td>\n",
       "      <td>-60.665222</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-28.436951</td>\n",
       "      <td>-59.213024</td>\n",
       "      <td>-38.408428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71916 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1     2         3          4          5          6     7   \\\n",
       "0       0  0.000000   0.0  0.000000   0.000000   0.000000   0.000000   0.0   \n",
       "1       0  0.000000   0.0  0.000000   0.000000   0.000000   0.000000   0.0   \n",
       "2       0  0.000000   0.0  0.000000   0.000000   0.000000   0.000000   0.0   \n",
       "3       0  0.000000   0.0  0.000000   0.000000   0.000000   0.000000   0.0   \n",
       "4       0  0.000000   0.0  0.000000   0.000000   0.000000   0.000000   0.0   \n",
       "...    ..       ...   ...       ...        ...        ...        ...   ...   \n",
       "71911 -80 -0.035597 -80.0 -8.098465  -8.980048 -15.847272 -10.322305 -80.0   \n",
       "71912 -80 -0.094599 -80.0 -7.325050 -11.943776 -17.820580  -8.446717 -80.0   \n",
       "71913 -80 -0.152973 -80.0 -6.007671 -13.862395 -17.086370  -8.125309 -80.0   \n",
       "71914 -80 -0.184401 -80.0 -6.244070 -38.443878 -58.667614  -9.242589 -80.0   \n",
       "71915 -80 -0.188906 -80.0 -6.216877 -57.667927 -57.141754  -8.873954 -80.0   \n",
       "\n",
       "              8          9   ...         31         32         33         34  \\\n",
       "0       0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000   \n",
       "1       0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000   \n",
       "2       0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000   \n",
       "3       0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000   \n",
       "4       0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000   \n",
       "...          ...        ...  ...        ...        ...        ...        ...   \n",
       "71911 -11.243167 -36.083233  ... -15.291732 -12.942391 -36.452793 -17.051119   \n",
       "71912 -11.624997 -12.545907  ... -19.534180 -16.903601 -36.371616 -16.394320   \n",
       "71913 -11.100876 -13.036957  ... -24.564272 -39.630966 -80.000000 -38.364044   \n",
       "71914 -10.375988 -13.404880  ... -16.364969 -35.572617 -36.778931 -16.553949   \n",
       "71915  -9.454443 -13.660504  ... -34.978451 -13.844250 -36.613995 -36.456799   \n",
       "\n",
       "              35         36         37         38         39  40  \n",
       "0       0.000000   0.000000   0.000000   0.000000   0.000000   0  \n",
       "1       0.000000   0.000000   0.000000   0.000000   0.000000   0  \n",
       "2       0.000000   0.000000   0.000000   0.000000   0.000000   0  \n",
       "3       0.000000   0.000000   0.000000   0.000000   0.000000   0  \n",
       "4       0.000000   0.000000   0.000000   0.000000   0.000000   0  \n",
       "...          ...        ...        ...        ...        ...  ..  \n",
       "71911 -60.872829 -80.000000 -58.338337 -41.416588 -59.643997   0  \n",
       "71912 -42.433369 -59.528172 -61.072720 -60.711777 -40.487423   0  \n",
       "71913 -43.945827 -17.266998 -39.512138 -80.000000 -39.440868   0  \n",
       "71914 -64.693108 -59.632282 -43.261234 -80.000000 -80.000000   0  \n",
       "71915 -60.665222 -80.000000 -28.436951 -59.213024 -38.408428   0  \n",
       "\n",
       "[71916 rows x 41 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=6):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs) \n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs) \n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  \n",
    "        ## (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    ## For calling multihead attention on embedded data and arranging it sequentially and adding other layers.\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    ## For preliminary token generation and embedding\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures=dffeatures.dropna(axis=0)\n",
    "X = dffeatures.to_numpy()\n",
    "[m,n]=X.shape\n",
    "features=[]\n",
    "for i in range(0,m):\n",
    "    features.append([X[i,0:n-2],X[i,n-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71911</th>\n",
       "      <td>[-80.0, -0.03559684753417969, -80.0, -8.098464...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71912</th>\n",
       "      <td>[-80.0, -0.0945994034409523, -80.0, -7.3250498...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71913</th>\n",
       "      <td>[-80.0, -0.1529731750488281, -80.0, -6.0076708...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71914</th>\n",
       "      <td>[-80.0, -0.1844005584716797, -80.0, -6.2440700...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71915</th>\n",
       "      <td>[-80.0, -0.1889057159423828, -80.0, -6.2168769...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature  class_label\n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
       "...                                                  ...          ...\n",
       "71911  [-80.0, -0.03559684753417969, -80.0, -8.098464...          0.0\n",
       "71912  [-80.0, -0.0945994034409523, -80.0, -7.3250498...          0.0\n",
       "71913  [-80.0, -0.1529731750488281, -80.0, -6.0076708...          0.0\n",
       "71914  [-80.0, -0.1844005584716797, -80.0, -6.2440700...          0.0\n",
       "71915  [-80.0, -0.1889057159423828, -80.0, -6.2168769...          0.0\n",
       "\n",
       "[71916 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 feature  class_label\n",
      "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
      "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
      "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
      "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
      "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0\n",
      "...                                                  ...          ...\n",
      "71911  [-80.0, -0.03559684753417969, -80.0, -8.098464...          0.0\n",
      "71912  [-80.0, -0.0945994034409523, -80.0, -7.3250498...          0.0\n",
      "71913  [-80.0, -0.1529731750488281, -80.0, -6.0076708...          0.0\n",
      "71914  [-80.0, -0.1844005584716797, -80.0, -6.2440700...          0.0\n",
      "71915  [-80.0, -0.1889057159423828, -80.0, -6.2168769...          0.0\n",
      "\n",
      "[71916 rows x 2 columns]\n",
      "8000000\n",
      "[[8000000 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " [8000000 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " [8000000 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " ...\n",
      " [      0 7984702       0 ... 6273300 4048786       0]\n",
      " [      0 7981559       0 ... 2036771 3673876       0]\n",
      " [      0 7981109       0 ...       0 5156304 2078697]] (71916, 39)\n",
      "[0. 0. 0. ... 0. 0. 0.] (71916,)\n"
     ]
    }
   ],
   "source": [
    "featuresdf=featuresdf.dropna(axis=0)\n",
    "print(featuresdf)\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "X=X*100000\n",
    "min_X=-min([min(element) for element in X])\n",
    "x=X+min_X\n",
    "x=x.astype(int)\n",
    "max_len=max([max(element) for element in x])\n",
    "print(max_len)\n",
    "# Getting label size\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "print(x,x.shape)\n",
    "print(y,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71916 39\n",
      "(71916,)\n"
     ]
    }
   ],
   "source": [
    "m,n=x.shape\n",
    "print(m,n)\n",
    "print(y.shape)\n",
    "x_train=x[0:int(m*9/10),:]\n",
    "x_test=x[int(m*9/10):m,:]\n",
    "y_train=y[0:int(m*9/10)]\n",
    "y_test=y[int(m*9/10):m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192 39\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'int'>\n",
      "<class 'list'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.ndarray'>\n",
      "64724 39\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'int'>\n",
      "<class 'list'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.ndarray'>\n",
      "(64724,) (64724,) (7192,) (7192,)\n"
     ]
    }
   ],
   "source": [
    "m,n=x_test.shape\n",
    "print(m,n)\n",
    "\n",
    "## Converting form and reshaping\n",
    "TestX=x_test\n",
    "TestY=y_test\n",
    "testy=np.reshape(TestY,(m,))\n",
    "\n",
    "## Changing datatypes\n",
    "testx=np.empty((m,),object)\n",
    "for i in range (0,m):\n",
    "    testx[i]=list(int(v) for v in TestX[i])\n",
    "    testy[i]=testy[i].astype(int)\n",
    "\n",
    "## Printing data-types - relevant to transformer input\n",
    "print(type(testx))\n",
    "print(type(testx[m-1][n-1]))\n",
    "print(type(testx[m-1]))\n",
    "print(type(testy[m-1]))\n",
    "print(type(testy))\n",
    "\n",
    "## Converting Train Data and Getting size of data\n",
    "m,n=x_train.shape\n",
    "print(m,n)\n",
    "\n",
    "## Converting form and reshaping\n",
    "TrainX=x_train\n",
    "TrainY=y_train\n",
    "trainy=np.reshape(TrainY,(m,))\n",
    "\n",
    "## Changing datatypes\n",
    "trainx=np.empty((m,),object)\n",
    "for i in range (0,m):\n",
    "    trainx[i]=list(int(v) for v in TrainX[i])\n",
    "    trainy[i]=TrainY[i].astype(int)\n",
    "    \n",
    "## Printing data-types - relevant to transformer input\n",
    "print(type(trainx))\n",
    "print(type(trainx[m-1][n-1]))\n",
    "print(type(trainx[m-1]))\n",
    "print(type(trainy[m-1]))\n",
    "print(type(trainy))\n",
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64724 Training sequences\n",
      "7192 Validation sequences\n",
      "(64724, 40) (64724,) (7192, 40) (7192,)\n",
      "[[      0 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " [      0 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " [      0 8000000 8000000 ... 8000000 8000000 8000000]\n",
      " ...\n",
      " [      0       0 7941571 ... 2195945 6350569 6301741]\n",
      " [      0       0 7996376 ... 2171862 6221349 5976445]\n",
      " [      0       0 7981751 ... 6309285 3797697 3917446]] [0. 0. 0. ... 0. 0. 0.] [[      0       0 7952578 ... 6435506 4035311 6186128]\n",
      " [      0       0 7981283 ... 6499866 1902946 4320971]\n",
      " [      0       0 7969698 ... 4392839 2075936 6544528]\n",
      " ...\n",
      " [      0       0 7984702 ... 6273300 4048786       0]\n",
      " [      0       0 7981559 ... 2036771 3673876       0]\n",
      " [      0       0 7981109 ...       0 5156304 2078697]] [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max_len+1\n",
    "maxlen = 40\n",
    "print(len(trainx), \"Training sequences\")\n",
    "print(len(testx), \"Validation sequences\")\n",
    "\n",
    "## Converting to padded tensor sequence\n",
    "trainx = keras.preprocessing.sequence.pad_sequences(trainx,maxlen=maxlen)\n",
    "testx = keras.preprocessing.sequence.pad_sequences(testx,maxlen=maxlen)\n",
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)\n",
    "print(trainx,trainy,testx,testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 40, 30)            240001230 \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 40, 30)            5700      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 40, 6)             546       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 40, 30)            210       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 30)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 20, 30)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20, 30)            930       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 30)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 248       \n",
      "=================================================================\n",
      "Total params: 240,011,654\n",
      "Trainable params: 240,011,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=8000001\n",
    "maxlen=40\n",
    "embed_dim = 30  ## Embedding size for each token\n",
    "num_heads = 6  ## Number of attention heads\n",
    "ff_dim = 30  ## Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "## Tokenizing input data with max dimension and embedding it\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "#x = keras.Sequential()\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "## Adding Sequential layer to the embedded data and attention layers too.\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "\n",
    "## Add other layers\n",
    "x = layers.Conv1D(6,3,padding=\"same\")(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "## Producing general softmax layer for classification\n",
    "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "## Generating model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  4/648 [..............................] - ETA: 27:22 - loss: 2.2548 - accuracy: 0.0477"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1c0a111a0ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(trainx, trainy, batch_size=100, epochs=3,validation_data=(testx, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(trainx, trainy, verbose=0)\n",
    "print(\"Training Performance\",score)\n",
    "score = model.evaluate(testx, testy, verbose=0)\n",
    "print(\"Testing Performanr\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
